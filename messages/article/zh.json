{
  "Article1": {
    "heading": "校企共建人工智能创新中心",
    "p1": "华中科技大学与武汉钐秾科技有限公司近日签署合作共建人工智能创新中心(Artificial Intelligence Innovation Institute，简称AIII)，双方将结合研发、产业、人才、创新资源等优势，拓展人工智能技术在工业制造、数字内容、分布式存储、消防、医疗、泛娱乐等领域的应用。",
    "p2": "华科钐秾武汉人工智能创新中心依托华科电信学院成立，是以人工智能、机器学习和科学计算相关研究为核心的产学研复合体，主要开展人工智能相关领域的技术研发、系统研制、智能化升级改造及试验验证，推动创新成果转化和产业孵化，吸引和培养一流科研人才，促进科技协同创新和产业融合发展，致力于成为高水平的新型科研机构。",
    "p3": "据悉，依托武汉人工智能产业扶持政策以及高端产业聚集的区位优势，双方将共同打造武汉人工智能产业新高地，助推科技成果转化和高层次人才培养。",
    "p4": "目前创新中心已聚集了大量人工智能领域的精英与学者，拥有核心科学家团队成员10人，核心成员均有博士学位并具有长期在国外人工智能知名实验室的访问研究经历。创新中心还邀请了包括2名诺贝尔奖得主及1名 IEEE院士在内的多名外籍科学家作为顾问。此外还有人数众多的博士、硕士研究生团队参与科学研究和技术开发。",
    "p5": "人工智能创新中心的成立将加速武汉新型科技产业人才的集聚，推动区域数字化发展与产业升级，增强武汉科技自研创新综合实力，促进武汉人工智能领域研究向更前沿的阶段演进。"
  },
  "Article3": {
    "heading": "小说标签智能标注系统",
    "p1": "根据中国互联网络信息中心发布的第 48 次《中国互联网络发展状况统计报告》显示，截止2020年9月，我国网络文学用户规模达4.67亿，手机网络文学用户规模达4.65亿。虽然受国内互联网人口红利缓慢消逝的影响，网络文学用户增速相较此前数年均值有所放缓，但从数据表现来看行业用户规模依然呈现出稳定增长的健康发展态势。庞大的用户规模为网络文学的发展提供了流量基础，各大网文平台的大力扶持和投入导致近年来网文作品数量呈现井喷式增长。艾瑞咨询在2020年6月发布的《中国网络文学版权保护研究报告》中提到，阅文集团截至2019年入驻作家就多达810万位，平台作品总数量达到1220万部，自有原创文学作品1150万部，并不断有新作品涌现。",
    "p2": "网络文学作品基数大、更新快，为作者和读者都带来了一些困扰和挑战。对于网络文学作者来说，掌握当前流量导向以及读者感兴趣的作品类型，才能更好地指导写作方向，增长作品的点击量。对于读者来说，如何在庞杂的作品中选择自己感兴趣的是一个亟需解决的难题，很多读者往往只能从平台热门作品中选择，导致一些新晋的优秀作品越来越难获得点击，不利于整个行业的长期发展。同时，网络文学作品通常字数多、人物杂，读者很难梳理清楚文章的脉络以及人物之间的关系，常常读了就忘，导致作品的传播力受限。为了给作者提供写作指导，同时方便读者选择，作品分类是常用的解决方法。通过基于多个不同的维度对作品进行分类，便于后期的分析和可视化展示，也为作者和读者了解作品带来了便利。此外，通过文本摘要提取作品中的故事亮点，可以帮助读者提前了解故事的大致剧情。除了简单的作品分类之外，为了帮助读者理清故事的脉络和人物之间的关系，可以提取作者中出现的所有人物，从多个维度进行人物画像，进行人物性格和特点的描述。此外，通过人物关系图谱进行可视化展示，更有利于理解众多人物之间错综复杂的关系。",
    "p3": "网络小说的分析主要是文本的分析，传统的文本分析方法基于特征的方法，即基于文本提取词频、词性、TF-IDF等相关特征，然后采用支持向量机（Support Vector Machine, SVM）或者梯度提升树（Gradient Boosted Decision Tree, GBDT）的机器学习方法进行分类。传统的机器学习方法需要专家人工设计特征，人工特征训练的模型可解释性强，但是需要丰富的先验知识，并且需要根据不同的任务设计不同的特征，模型泛化能力比较一般。2012 年以来，随着深度学习技术以及GPU性能的快速发展，基于深度学习的自然语言处理技术逐渐取代了传统人工特征加机器学习的方法。基于深度学习的方法通常不需要人工特征，而是将每一个单词映射为稠密的嵌入表征，然后经过设计好的多层神经网络，计算损失函数并通过反向传播算法（Back-propagation algorithm）对神经网络以及嵌入表征的参数进行更新。",
    "header-main-1": "算法简介",
    "header-sub-1": "Word2Vec模型",
    "p4": "Word2Vec是一种在向量空间高效获取单词表征的方法。Word2Vec基于一个符合直觉的假设，即具有相似上下文的两个不同单词的语义是接近的。其核心思想是考虑到单词的语义受到其上下文环境的影响，所以可以通过单隐层的神经网络构建上下文和该单词之间的关联，这样通过大量无标注的语料训练这样的神经网络，隐层的参数即可作为对应索引单词的表征。Word2Vec模型定义了单隐层来构建一段语句中心词和周边词之间的关系，其中隐层的参数维度为，其中表示词表中单词的数量，表示输出词向量的维度。Word2Vec提出了两种网络的训练方法，即CBOW和Skip-Gram，它们的模型结构如图3‑1所示。",
    "p5": "CBOW模型的核心思想是使用中心词的周边词来预测该单词，通过定义一个固定长度的滑动窗口来获取中心词和周边词，其中周边词既包含上文单词也包含下文单词。将周边词作为输入层的输出，将周边词嵌入向量的平均值作为隐层的输出，最后计算输出层的得分和交叉熵损失进行反向传播。",
    "p6": "<p>如公式（3-1）所示，表示上下文单词的数量， C 表示上下文中单词对应的数量，<strong><i>X<sub>1</sub>,...,X<sub>C</sub></i></strong>&nbsp;&nbsp;表示上下文中单词对应的独热编码，然 <strong><i>V<sub>w1</sub></i></strong> 表示单词 <strong><i>W<sub>1</sub></i></strong> 的嵌入向量，目标函数定义为公式（3-2）。该目标函数的含义是基于给定的上下文单词 <strong><i>W<sub>1</sub>,...,W<sub>C</sub></i></strong> 最大化输出为中心词的 <strong><i>W<sub>0</sub></i></strong> 的概率：</p>",
    "p7": "<p>Skip-Gram模型的思想和CBOW恰好相反，将中心词作为输入表示为 <strong><i>W<sub>1</sub></i></strong>，上下文单词 <strong><i>W<sub>1</sub>,...,W<sub>C</sub></i></strong> 作为预测的目标输出。此时目标函数如公式（3-3）所示。</p>",
    "p8": "CBOW模型和Skip-Gram模型都基于大量无标注的语料，使用反向传播算法进行训练。通过反向传播不断更新隐层参数，确保单词对应的表征向量偏向于其最常用的语义。",
    "header-main-2": "距离向量",
    "p9": "<p>欧式距离是欧氏空间中两点间的距离。欧式距离是基础的度量距离，常用于度量两个向量间的距离。对于欧氏空间中的两个向量 <strong><i>X = (x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>)</i></strong> 和 <strong><i>Y = (y<sub>1</sub>,y<sub>2</sub>,...y<sub>n</sub>)</i></strong> ，在本系统中即对应两个文本嵌入式向量，它们的欧氏距离如式（3-4）所示。</p>",
    "p10": "余弦距离通过测量两个向量的夹角的余弦值来度量它们之间的距离，余弦距离也称为余弦相似度，常用于文本相似度的度量。对于两个文本嵌入式向量，它们的余弦距离如式（3-5）。",
    "header-main-3": "文本分类器",
    "p11": "RNN具有短期记忆功能，对处理具有序列结构的文本有着天然优势。RNN模型结构如图3‑2所示。",
    "caption": "图 3‑2  RNN模型结构图",
    "p12": "<p>根据上图3‑2所示，对文本序列的词嵌入表示 <strong><i>x<sub>0</sub>,x<sub>1</sub>,x<sub>2</sub>,...,x<sub>t</sub></i></strong>，在 <strong><i>t</i></strong> 时刻输入 <strong><i>x<sub>t</sub></i></strong> 和上一时刻的输出状态 <strong><i>h<sub>t-1</sub></i></strong> 来得到 <strong><i>t</i></strong> 时刻的输出，如公式（3-6）所示。</p>",
    "p13": "<p>其中， <strong><i>h<sub>t-1</sub></i></strong> 为当前时刻的隐层输出，函数 <strong><i>g</i></strong> 为激活函数， <strong><i>W<sub>h</sub></i></strong> 是将前一时刻的输出转化为当前时刻的输出的权重， <strong><i>W<sub>h</sub></i></strong> 是将输入 <strong><i>x</i></strong> 转换为输出 <strong><i>h</i></strong> 的权重，<strong><i>b</i></strong> 是偏置。</p>",
    "p14": "由图3‑2所示，各个时刻的RNN层接收该层的输入和前一个RNN层的输出，然后据此计算当前时刻的输出。因此，RNN是“具有记忆的层”，能够捕捉文本的序列信息。"
  }
}
